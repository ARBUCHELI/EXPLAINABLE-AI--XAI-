# EXPLAINABLE AI
----------------

* What is Explainable AI?
-------------------------
Learn about Explainable AI and its importance in understanding the inner workings of deep learning systems.

Key ideas:

	. Explainable AI (XAI) is a set of techniques used to make the inner workings of deep learning systems transparent and understandable to humans.
	----------------

	. Some AI algorithms, like decision trees, are called inherently explainable. Due to their structure, it is possible (at least in theory) for human experts to understand how these 
	algorithms make decisions.

	. Other AI algorithms, like deep neural networks, are inherently opaque. Sometimes called black boxes, these algorithms can be highly effective, but it is usually very hard to 
	understand how they are working.

	. The goal of Explainable AI is to make black boxes into glass boxes, providing transparency into the decision-making process of deep learning systems.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: why do you think we care about explainability at all, if the algorithms work?
---------

* See our answer
----------------
	. Imagine an algorithm being used to diagnose a medical condition. A human doctor has all sorts of factors that influence their decision, including judgement calls like "better to 
	err on the side of caution." If we want the algorithm to do the same, we need to know how the algorithm is making its decisions!

	. Algorithms are also only as good as their data. If the data being fed into the algorithm is biased, the algorithm may reproduce that bias. Medical datasets, for example, often 
	don't contain the same amount or quality of data for different types of bodies. So our medical diagnostic algorithm may be influenced by biases in its dataset. Without knowing how 
	it is making decisions, we will have a much harder time correcting for these issues.

	. For more details, see our article Dangers of the Black Box

- Explainable AI (XAI)
----------------------

	. Inherently Explainable Algorithms
	-----------------------------------
		. Decision trees
		. Regression algorithms
		. Bayesian classifiers
		. Support vector machines

	. Inherently Opaque Algorithms
	------------------------------
		. Deep Learning Algorithms
		. Genetic Algorithms
		

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* The Interpretability Problem
------------------------------
Learn about the interpretability problem and how deep learning systems store knowledge and make decisions.

Key ideas:

	. The interpretability problem is the challenge of explaining the inner workings of a deep learning system.

	. In a deep learning system, knowledge implicit in the training data is reflected in changes to the structure of its hidden layers.

	. Different inputs activate different groups of neurons or activation paths within these layers, resulting in different outputs or decisions.
	
	. So understanding the structure of hidden layers can help us understand how a deep learning system stores knowledge and makes decisions.

	. There are three main approaches to studying deep learning systems: constraint, perturbation, and generative adversarial networks (GANS). We’ll learn more about each of these in 
	later videos.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: how does the example deep learning system store conceptual information, like the idea of a “triangle”?

* See our answer
----------------
Concepts correspond to groups of nodes that are activated by a class of inputs. For example, the group of nodes that is activated whenever a triangle is fed into the neural network 
represent the concept of "triangle". By understanding how different inputs activate different groups of neurons, we can gain insight into where and how concepts are stored inside the 
hidden layers, and begin to explain how the deep learning system makes its decisions!


------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Right to Explanation
----------------------
Learn about right to explanation regulations and their real-world applications.

Key ideas:

	. Algorithms need to be regulated to ensure they meet social, ethical, and legal standards.

	. The right to explanation is a legal right that guarantees individuals receive a clear explanation of algorithmic outputs.

	. Several such laws are already on the books, including
	
		. the Equal Credit Opportunity Act (in the USA)

		. the General Data Protection Regulation (in the EU)

		. the Digital Republic Act (in France)

	. For example, the GDPR includes the right to an explanation of the reasons behind legal or financial decisions made by automated systems.

	. Expect many more such laws as AI systems become increasingly embedded in society.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: why is a right to explanation necessary?
---------

* See our answer
----------------
Deep learning systems are becoming more and more prevalent, and their decisions impact human beings. In worst case scenarios, like places that use AI in criminal justice, these decisions 
could have serious impacts on people's lives and human rights. Anyone impacted like this should be able to receive an actual explanation of how that decision was made. And as a society, 
we need these explanations to make sure AI systems aren't violating human rights or otherwise making incorrect decisions.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------

* Counterfactual Method
-----------------------
Learn about the counterfactual method for interpreting neural networks.

Key ideas:

	. Counterfactuals are hypothetical situations used to test a neural network’s decision-making boundaries.

	. For example, imagine a neural network that is supposed to detect fakes of the Mona Lisa. As a counterfactual, we might present the neural network with an image of the real Mona 
	Lisa, but with added eyelashes.

	. How the neural network classifies this image will help us understand whether “has eyelashes” is a feature it is using to determine real from fake.

	. The counterfactual method is a perturbation approach, meaning that it makes small changes to the input and looks at impacts on the output.

	. The counterfactual method requires a lot of input and testing to obtain a clear picture of the network’s inner workings.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: why does the counterfactual method require a lot of input and testing?
---------

* See our answer
----------------
Each individual counterfactual only tells us a little bit about the neural network. For example, if the neural network classifies our Mona Lisa With Eyelashes as fake, that only tells us 
that the neural network's concept of "real Mona Lisa" does not include eyelashes. In order to understand the full picture, we have to change just one aspect of the Mona Lisa at a time 
over and over to detect which features the neural network "cares" about in determining real from fake.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Axiomatic Attribution
-----------------------
Learn about axiomatic attribution and how it can be applied to recognizing the Mona Lisa.

Key ideas:

	. Axiomatic attribution involves building an input step by step and checking the output at each step.

	. Think about it like a game of pictionary. For the Mona Lisa, we construct sketches of the Mona Lisa stroke by stroke.

	. The goal is to determine the minimum information needed for the network to recognize the real Mona Lisa.

	. Axiomatic attribution is a perturbation approach that adds information with each step to test decision boundaries.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: what is the advantage of using axiomatic attribution over the counterfactual method?
---------

* See our answer
----------------
Both methods are perturbation methods, where we vary inputs slightly and see how the output changes. However, axiomatic attribution in general requires significantly fewer inputs and 
testing than the counterfactual method.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Intelligible Models
---------------------
Learn about the intelligible models approach.

Key ideas:

	. The intelligible models approach is based on statistical analysis.

	. It works by varying statistical properties of inputs and seeing how that impacts the output.

	. In the Mona Lisa case, we might do a statistical analysis of its “chemical signature”: the different types of paints and materials across the canvas.

	. We could then create fakes that vary this chemical signature ever so slightly and test the outputs.

	. The degree of similarity between fakes can be measured to determine how far variations can be taken before it is identified as a fake.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: is intelligible models a reasonable approach for our Mona Lisa example?
---------

* See our answer
----------------
Intelligible models is probably not the best or most natural approach in this case, since it requires creating actual physical fakes, instead of just digital ones.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Monotonicity
--------------
Learn about the concept of monotonicity and its application in AI interpretability.

Key ideas:

	. Neural networks with monotonicity tend to emphasize similarities instead of detecting differences

	. This contrasts with adversarial networks, which emphasize minute differences in inputs.

	. Note that the same network can exhibit aspects of both monotonic and adversarial behavior.

	. In our Mona Lisa example, a network that is more monotonic will look for major features (like the mysterious smile) to classify painting in a Mona Lisa style as Mona Lisas. A 
	more adversarial network might find minute differences in these and reject them.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: how does monotonicity impact interpretability?
---------

* See our answer
----------------
Whether we want a more adversarial or monotonic network depends on the problem we are solving with the AI. Do we want a network that will tell us if we have the real Mona Lisa, or a 
network that will tell us if a painting is a version of the Mona Lisa? Knowing this can guide our approach to interpretability. In the former case, we want to know which minute 
differences cause the network to reject a fake. In the latter case, we want to know what are the broad categories that make up the conceptual space of "like a Mona Lisa".

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Rationalization
-----------------
Learn about the interpretability technique of rationalization.

Key ideas:

	. Rationalization is an AI interpretability method that allows the AI to explain its decision-making process.

	. It involves capturing data from a human expert and encoding it into a deep learning network.

	. When the network is presented with a task, it activates certain nodes and paths that trigger the expert’s verbal output associated with them.

	. This allows the AI to rationalize its conclusions in terms that humans can understand.

	. Rationalization gets us closer to the dream of having AI that can explain its decision-making process like a human.

* Instructions
--------------
Think about the following question as you watch the video. When you are ready to move on, select Next.

Question: how is rationalization different from all the prior techniques?
---------

* See our answer
----------------
All our prior techniques have involved some kind of perturbation: changing inputs and seeing how that changes the outputs. Essentially, we are trying to piece together the workings of the 
black box by doing a series of scientific experiments. With rationalization, the black box is (in an ideal world) directly telling us how it is working!

------------------------------------------------------------------------------------------------------------------------------------------------------------------




































































































































































































